{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbXkUQWFLBRF"
      },
      "source": [
        "# HW2P2: Image Recognition and Verification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "695K5zs36a48"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQr0ss8w6jVI"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi # Run this to see what GPU you have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmbTatic6PDX"
      },
      "outputs": [],
      "source": [
        "!pip install wandb --quiet # Install WandB\n",
        "!pip install pytorch_metric_learning --quiet #Install the Pytorch Metric Library\n",
        "!pip install torchsummaryX==1.1.0 wandb --quiet\n",
        "!pip install torchvision --quiet\n",
        "!pip install --upgrade kaggle==1.6.17 --force-reinstall --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_oCzGBTh6xjL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummaryX import summary\n",
        "import torchvision\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.v2 as T\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics as mt\n",
        "from scipy.optimize import brentq\n",
        "from scipy.interpolate import interp1d\n",
        "import glob\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_metric_learning import samplers, losses\n",
        "\n",
        "import csv\n",
        "import random\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT9MZk9p69Q5",
        "outputId": "5187e528-3105-45b7-fd38-2643140e468d"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive # Link to your drive if you are not using Colab with GCP\n",
        "# drive.mount('/content/drive') # Models in this HW take a long time to get trained and make sure to save it here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf6Da1K37BSJ"
      },
      "source": [
        "# Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # change in kaggle\n",
        "# data_dir = '/kaggle/input/11785-hw-2-p-2-face-verification-spring-2025/HW2p2_S25'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "id": "sNRYbTmU7Dk3"
      },
      "outputs": [],
      "source": [
        "# # run on mac\n",
        "# !mkdir 'content/data'\n",
        "# !kaggle competitions download -c 11785-hw-2-p-2-face-verification-spring-2025\n",
        "# !unzip -qo '11785-hw-2-p-2-face-verification-spring-2025' -d 'content/data'\n",
        "data_dir = 'content/data/HW2p2_S25' # change in kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OgkfYwP7HVt"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMXkHmFc7G9m"
      },
      "outputs": [],
      "source": [
        "!mkdir 'checkpoint'\n",
        "\n",
        "config = {\n",
        "    # machine resources\n",
        "    'num_workers': 5, # change in kaggle\n",
        "    'pin_memory': True, # change in kaggle\n",
        "\n",
        "    # data paths\n",
        "    'cls_data_dir': os.path.join(data_dir, \"cls_data\"), #TODO: Provide path of classification directory\n",
        "    'ver_data_dir': os.path.join(data_dir, \"ver_data\"), #TODO: Provide path of verification directory\n",
        "    'val_pairs_file': os.path.join(data_dir,\"val_pairs.txt\"), #TODO: Provide path of text file containing val pairs for verification\n",
        "    'test_pairs_file': os.path.join(data_dir,\"test_pairs.txt\"), #TODO: Provide path of text file containing test pairs for verification\n",
        "    \n",
        "    # checkpoint paths\n",
        "    'checkpoint_dir': \"checkpoint\", #TODO: Checkpoint directory\n",
        "    \n",
        "    # data size, change in kaggle\n",
        "    'num_classes': 8631, #Dataset contains 8631 classes for classification, reduce this number if you want to train on a subset, but only for train dataset and not on val dataset\n",
        "    \n",
        "    # data augmentation\n",
        "    'augment': True,\n",
        "    'crop': {'if_on': True, 'scale': (0.8, 1.2)},\n",
        "    'flip': {'if_on': True, 'probability': 0.5},\n",
        "    'rotation': {'if_on': True, 'degrees': 15},\n",
        "    'colorjitter': {'if_on': False, 'brightness': (0.9, 1.1), 'contrast': (0.9, 1.1), 'saturation': (0.9, 1.1), 'hue': (-0.02, 0.02)},\n",
        "    'greyscale': {'if_on': False, 'probability': 0.05},\n",
        "    'affine': {'if_on': False, 'degrees': 0, 'translate': (0.1, 0.1), 'scale': (0.9, 1.1)},\n",
        "    'perspective': {'if_on': False, 'distortion_scale': 0.2, 'probability': 0.2},\n",
        "    'normalize_mean': [0.5, 0.5, 0.5], \n",
        "    'normalize_std': [0.5, 0.5, 0.5], \n",
        "\n",
        "    # model\n",
        "    'model': 'MobileFaceNet', # ['Network', 'DeeperNetwork', 'ConvNeXt', 'ResNet18', 'ResNet34', 'MobileFaceNetLike', 'EfficientNetB5Face']\n",
        "    'model_params': {'dropout_rate': 0.2, \"activation\": 'prelu', \"use_attention\": True, \"channel_reduction\": 4, \"spatial_kernel_size\": 7}, # {'dropout_rate': 0.2, \"width_mult\": 0.8},\n",
        "    'model_train': {\"use_adversarial_training\": False, \"adversarial_training_alpha\": 0.05, \"adversarial_training_beta\": 0.05},\n",
        "    'epochs': 60, # 20 epochs is recommended ONLY for the early submission - you will have to train for much longer typically.\n",
        "    'batch_size': 256, # Increase this if your GPU can handle it\n",
        "\n",
        "    # loss\n",
        "    'loss': 'arcface', # [None, 'triplet', 'npair', 'cosface', 'arcface', 'combined']\n",
        "    # cross entropy\n",
        "    'label_smoothing': 0.1, # Label smoothing for classification loss\n",
        "    'cross_entropy_weight': 1,\n",
        "    'other_loss_weight': 1.5,\n",
        "    # triplet\n",
        "    'margin': 0.2,\n",
        "    # npair\n",
        "    # cosface (scale, additive_margin), arcface (scale, angular_margin), combined (scale, angular_margin, additive_margin)\n",
        "    'scale': 30,\n",
        "    'additive_margin': 0.5,\n",
        "    'angular_margin': 0.5,\n",
        "    'combined_arcface_weight': 0.5,\n",
        "\n",
        "    # optimizer\n",
        "    'lr': 1e-3,\n",
        "    'weight_decay': 1e-4,\n",
        "    # 'scheduler': 'CosineAnnealingWarmRestarts',\n",
        "    # 'scheduler_params': {'T_0': 50, 'T_mult': 1, 'eta_min': 1e-8},\n",
        "    'scheduler': 'CosineAnnealingLR',\n",
        "    'scheduler_params': {'T_max': 100, 'eta_min': 1e-9},\n",
        "\n",
        "    # wandb\n",
        "    'wandb_name': 'aug(norm0.5+rotation+flip0.5+crop1.2), model(MobileFaceNet,attention), loss(ce0.2+arcface30+0.5(1.5)), lr(3-9,l00), bs(256)',\n",
        "    'wandb_init': True,\n",
        "    'wandb_id': None,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEAW65sB8Wlp"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "x-jrgnbQyR2s"
      },
      "outputs": [],
      "source": [
        "def create_transforms(image_size: int = 112, augment: bool = True) -> T.Compose:\n",
        "    \"\"\"Create transform pipeline for face recognition.\"\"\"\n",
        "\n",
        "    # Step 1: Basic transformations\n",
        "    transform_list = [\n",
        "        # Resize the image to the desired size (image_size x image_size)\n",
        "        T.Resize((image_size, image_size)),\n",
        "\n",
        "        # Convert PIL Image to tensor\n",
        "        T.ToTensor(),\n",
        "\n",
        "        # Convert image to float32 and scale the pixel values to [0, 1]\n",
        "        T.ToDtype(torch.float32, scale=True),\n",
        "    ]\n",
        "\n",
        "    # Step 2: Data augmentation (optional, based on `augment` argument)\n",
        "    if augment:  # This block will be executed if `augment=True`\n",
        "        transform_list.extend([\n",
        "            transforms.RandomResizedCrop(size=image_size, scale=config['crop']['scale']) if config['crop']['if_on'] else None, \n",
        "            transforms.RandomHorizontalFlip(p=config['flip']['probability']) if config['flip']['if_on'] else None,\n",
        "            transforms.RandomRotation(degrees=config['rotation']['degrees']) if config['rotation']['if_on'] else None,\n",
        "            transforms.ColorJitter(brightness=config['colorjitter']['brightness'], contrast=config['colorjitter']['contrast'], saturation=config['colorjitter']['saturation'], hue=config['colorjitter']['hue']) if config['colorjitter']['if_on'] else None,\n",
        "            transforms.RandomGrayscale(p=config['greyscale']['probability']) if config['greyscale']['if_on'] else None,\n",
        "            torchvision.transforms.RandomAffine(degrees=config['affine']['degrees'], translate=config['affine']['translate'], scale=config['affine']['scale']) if config['affine']['if_on'] else None,\n",
        "            torchvision.transforms.RandomPerspective(distortion_scale=config['perspective']['distortion_scale'], p=config['perspective']['probability']) if config['perspective']['if_on'] else None,\n",
        "        ])\n",
        "\n",
        "    # Step 3: Standard normalization for image recognition tasks\n",
        "    # The Normalize transformation requires mean and std values for each channel (R, G, B).\n",
        "    # Here, we are normalizing the pixel values to have a mean of 0.5 and std of 0.5 for each channel.\n",
        "    transform_list.extend([\n",
        "        T.Normalize(config['normalize_mean'], config['normalize_std'])  # Standard mean and std for face recognition tasks\n",
        "    ])\n",
        "\n",
        "    # Remove None values from the list\n",
        "    transform_list = [t for t in transform_list if t is not None]\n",
        "        \n",
        "    # Return the composed transformation pipeline\n",
        "    return T.Compose(transform_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzxNNzGe8ccB"
      },
      "source": [
        "### Classification Datasets and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6Nn57B-7F-i"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Custom dataset for loading image-label pairs.\"\"\"\n",
        "    def __init__(self, root, transform, num_classes=config['num_classes']):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root (str): Path to the directory containing the images folder.\n",
        "            transform (callable): Transform to be applied to the images.\n",
        "            num_classes (int, optional): Number of classes to keep. If None, keep all classes.\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.labels_file = os.path.join(self.root, \"labels.txt\")\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.classes = set()\n",
        "\n",
        "        # Read image-label pairs from the file\n",
        "        with open(self.labels_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        lines = sorted(lines, key=lambda x: int(x.strip().split(' ')[-1]))\n",
        "\n",
        "        # Get all unique labels first\n",
        "        all_labels = sorted(set(int(line.strip().split(' ')[1]) for line in lines))\n",
        "\n",
        "         # Select subset of classes if specified\n",
        "        if num_classes is not None:\n",
        "            selected_classes = set(all_labels[:num_classes])\n",
        "        else:\n",
        "            selected_classes = set(all_labels)\n",
        "\n",
        "        # Store image paths and labels with a progress bar\n",
        "        for line in tqdm(lines, desc=\"Loading dataset\"):\n",
        "            img_path, label = line.strip().split(' ')\n",
        "            label = int(label)\n",
        "\n",
        "            # Only add if label is in selected classes\n",
        "            if label in selected_classes:\n",
        "                self.image_paths.append(os.path.join(self.root, 'images', img_path))\n",
        "                self.labels.append(label)\n",
        "                self.classes.add(label)\n",
        "\n",
        "        assert len(self.image_paths) == len(self.labels), \"Images and labels mismatch!\"\n",
        "\n",
        "        # Convert classes to a sorted list\n",
        "        self.classes = sorted(self.classes)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples.\"\"\"\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            idx (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (transformed image, label)\n",
        "        \"\"\"\n",
        "        # Load and transform image on-the-fly\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3d1tsYE0pb-"
      },
      "outputs": [],
      "source": [
        "# train transforms\n",
        "train_transforms = create_transforms(augment=config['augment'])\n",
        "\n",
        "# val transforms\n",
        "val_transforms   = create_transforms(augment=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOJOrQLX02Gh"
      },
      "outputs": [],
      "source": [
        "cls_train_dataset = ImageDataset(root = os.path.join(config['cls_data_dir'],'train'), transform=train_transforms, num_classes=config['num_classes'])\n",
        "cls_val_dataset   = ImageDataset(root = os.path.join(config['cls_data_dir'],'dev'), transform=val_transforms, num_classes=config['num_classes'])\n",
        "cls_test_dataset  = ImageDataset(root = os.path.join(config['cls_data_dir'],'test'), transform=val_transforms, num_classes=config['num_classes'])\n",
        "\n",
        "# assert cls_train_dataset.classes == cls_val_dataset.classes == cls_test_dataset.classes, \"Class mismatch!\"\n",
        "\n",
        "# Check dataset sizes\n",
        "print(f\"Training set size: {len(cls_train_dataset)}\")\n",
        "print(f\"Validation set size: {len(cls_val_dataset)}\")\n",
        "print(f\"Test set size: {len(cls_test_dataset)}\")\n",
        "\n",
        "# Dataloaders\n",
        "cls_train_loader = DataLoader(cls_train_dataset, batch_size=config['batch_size'], shuffle=True,  num_workers=config['num_workers'], pin_memory=config['pin_memory'])\n",
        "cls_val_loader   = DataLoader(cls_val_dataset,   batch_size=config['batch_size'], shuffle=False, num_workers=config['num_workers'], pin_memory=config['pin_memory'])\n",
        "cls_test_loader  = DataLoader(cls_test_dataset,  batch_size=config['batch_size'], shuffle=False, num_workers=config['num_workers'], pin_memory=config['pin_memory'])\n",
        "\n",
        "# Check if the dataloader is working\n",
        "for images, labels in cls_train_loader:\n",
        "    print(images.shape, labels.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPSk8DyK8htk"
      },
      "source": [
        "### Verification Dataset and Datatloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ImagePairDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Custom dataset for loading and transforming image pairs.\"\"\"\n",
        "    def __init__(self, root, pairs_file, transform):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root (str): Path to the directory containing the images.\n",
        "            pairs_file (str): Path to the file containing image pairs and match labels.\n",
        "            transform (callable): Transform to be applied to the images.\n",
        "        \"\"\"\n",
        "        self.root      = root\n",
        "        self.transform = transform\n",
        "\n",
        "        self.matches     = []\n",
        "        self.image1_list = []\n",
        "        self.image2_list = []\n",
        "\n",
        "        # Read and load image pairs and match labels\n",
        "        with open(pairs_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in tqdm(lines, desc=\"Loading image pairs\"):\n",
        "            img_path1, img_path2, match = line.strip().split(' ')\n",
        "            img1 = Image.open(os.path.join(self.root, img_path1)).convert('RGB')\n",
        "            img2 = Image.open(os.path.join(self.root, img_path2)).convert('RGB')\n",
        "\n",
        "            self.image1_list.append(img1)\n",
        "            self.image2_list.append(img2)\n",
        "            self.matches.append(int(match))  # Convert match to integer\n",
        "\n",
        "        assert len(self.image1_list) == len(self.image2_list) == len(self.matches), \"Image pair mismatch\"\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples.\"\"\"\n",
        "        return len(self.image1_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            idx (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (transformed image1, transformed image2, match label)\n",
        "        \"\"\"\n",
        "        img1 = self.image1_list[idx]\n",
        "        img2 = self.image2_list[idx]\n",
        "        match = self.matches[idx]\n",
        "        return self.transform(img1), self.transform(img2), match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "BxXF96_Ys8os"
      },
      "outputs": [],
      "source": [
        "class TestImagePairDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Custom dataset for loading and transforming image pairs.\"\"\"\n",
        "    def __init__(self, root, pairs_file, transform):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root (str): Path to the directory containing the images.\n",
        "            pairs_file (str): Path to the file containing image pairs and match labels.\n",
        "            transform (callable): Transform to be applied to the images.\n",
        "        \"\"\"\n",
        "        self.root      = root\n",
        "        self.transform = transform\n",
        "\n",
        "        self.image1_list = []\n",
        "        self.image2_list = []\n",
        "\n",
        "        # Read and load image pairs and match labels\n",
        "        with open(pairs_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in tqdm(lines, desc=\"Loading image pairs\"):\n",
        "            img_path1, img_path2 = line.strip().split(' ')\n",
        "            img1 = Image.open(os.path.join(self.root, img_path1)).convert('RGB')\n",
        "            img2 = Image.open(os.path.join(self.root, img_path2)).convert('RGB')\n",
        "\n",
        "            self.image1_list.append(img1)\n",
        "            self.image2_list.append(img2)\n",
        "\n",
        "        assert len(self.image1_list) == len(self.image2_list), \"Image pair mismatch\"\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples.\"\"\"\n",
        "        return len(self.image1_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            idx (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (transformed image1, transformed image2, match label)\n",
        "        \"\"\"\n",
        "        img1 = self.image1_list[idx]\n",
        "        img2 = self.image2_list[idx]\n",
        "        return self.transform(img1), self.transform(img2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ALvOc6L2r3d"
      },
      "outputs": [],
      "source": [
        "# Datasets\n",
        "ver_val_dataset  = ImagePairDataset(root=config['ver_data_dir'], pairs_file=config['val_pairs_file'], transform=val_transforms)\n",
        "ver_test_dataset = TestImagePairDataset(root=config['ver_data_dir'], pairs_file=config['test_pairs_file'], transform=val_transforms)\n",
        "\n",
        "# Check dataset sizes\n",
        "print(f\"Validation set size: {len(ver_val_dataset)}\")\n",
        "print(f\"Test set size: {len(ver_test_dataset)}\")\n",
        "\n",
        "# Dataloader\n",
        "ver_val_loader   = DataLoader(ver_val_dataset,  batch_size=config['batch_size'], shuffle=False, num_workers=config['num_workers'], pin_memory=config['pin_memory'])\n",
        "ver_test_loader  = DataLoader(ver_test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=config['num_workers'], pin_memory=config['pin_memory'])\n",
        "\n",
        "# Check if the dataloader is working\n",
        "for images1, images2, labels in ver_val_loader:\n",
        "    print(images1.shape, images2.shape, labels.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "436KzM6u-3A2"
      },
      "source": [
        "# EDA and Viz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niSJ49lzpHgW"
      },
      "source": [
        "### Classification Dataset Viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_cls_dataset_samples(train_loader, val_loader, test_loader, samples_per_set=8, figsize=(10, 6)):\n",
        "    \"\"\"\n",
        "    Display samples from train, validation, and test datasets side by side\n",
        "\n",
        "    Args:\n",
        "        train_loader: Training data loader\n",
        "        val_loader: Validation data loader\n",
        "        test_loader: Test data loader\n",
        "        samples_per_set: Number of samples to show from each dataset\n",
        "        figsize: Figure size (width, height)\n",
        "    \"\"\"\n",
        "    def denormalize(x):\n",
        "        \"\"\"Denormalize images from [-1, 1] to [0, 1]\"\"\"\n",
        "        return x * 0.5 + 0.5\n",
        "\n",
        "    def get_samples(loader, n):\n",
        "        \"\"\"Get n samples from a dataloader\"\"\"\n",
        "        # torch.manual_seed(39)\n",
        "        batch = next(iter(loader))\n",
        "        return batch[0][:n], batch[1][:n]\n",
        "\n",
        "    # Get samples from each dataset\n",
        "    train_imgs, train_labels = get_samples(train_loader, samples_per_set)\n",
        "    val_imgs, val_labels = get_samples(val_loader, samples_per_set)\n",
        "    test_imgs, test_labels = get_samples(test_loader, samples_per_set)\n",
        "\n",
        "    # Create figure\n",
        "    fig, axes = plt.subplots(3, 1, figsize=figsize)\n",
        "\n",
        "    # Plot each dataset\n",
        "    for idx, (imgs, labels, title) in enumerate([\n",
        "        (train_imgs, train_labels, 'Training Samples'),\n",
        "        (val_imgs, val_labels, 'Validation Samples'),\n",
        "        (test_imgs, test_labels, 'Test Samples')\n",
        "    ]):\n",
        "\n",
        "        # Create grid of images\n",
        "        grid = make_grid(denormalize(imgs), nrow=8, padding=2)\n",
        "\n",
        "        # Display grid\n",
        "        axes[idx].imshow(grid.permute(1, 2, 0).cpu())\n",
        "        axes[idx].axis('off')\n",
        "        axes[idx].set_title(title, fontsize=10)\n",
        "\n",
        "        # Add class labels below images (with smaller font)\n",
        "        grid_width = grid.shape[2]\n",
        "        imgs_per_row = min(8, samples_per_set)\n",
        "        img_width = grid_width // imgs_per_row\n",
        "\n",
        "        for i, label in enumerate(labels):\n",
        "            col = i % imgs_per_row  # Calculate column position\n",
        "            if label<len(train_loader.dataset.classes):\n",
        "              class_name = train_loader.dataset.classes[label]\n",
        "            else:\n",
        "              class_name = f\"Class {label} (Unknown)\"\n",
        "            axes[idx].text(col * img_width + img_width/2,\n",
        "                         grid.shape[1] + 5,\n",
        "                         class_name,\n",
        "                         ha='center',\n",
        "                         va='top',\n",
        "                         fontsize=6,\n",
        "                         rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_cls_dataset_samples(cls_train_loader, cls_val_loader, cls_test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZPVGXOu59Wh"
      },
      "source": [
        "### Ver Dataset Viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pBN7Z9K5iAM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "def show_ver_dataset_samples(val_loader, samples_per_set=4, figsize=(12, 8)):\n",
        "    \"\"\"\n",
        "    Display verification pairs from the validation dataset\n",
        "\n",
        "    Args:\n",
        "        val_loader: Validation data loader\n",
        "        samples_per_set: Number of pairs to show from the dataset\n",
        "        figsize: Figure size (width, height)\n",
        "    \"\"\"\n",
        "    def denormalize(x):\n",
        "        \"\"\"Denormalize images from [-1, 1] to [0, 1]\"\"\"\n",
        "        return x * 0.5 + 0.5\n",
        "\n",
        "    def get_samples(loader, n):\n",
        "        \"\"\"Get n samples from a dataloader\"\"\"\n",
        "        batch = next(iter(loader))\n",
        "        return batch[0][:n], batch[1][:n], batch[2][:n]\n",
        "\n",
        "    # Get samples from the validation dataset\n",
        "    val_imgs1, val_imgs2, val_labels = get_samples(val_loader, samples_per_set)\n",
        "\n",
        "    # Create figure and axis\n",
        "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
        "\n",
        "    # Create grids for both images in each pair\n",
        "    grid1 = make_grid(denormalize(val_imgs1), nrow=samples_per_set, padding=2)\n",
        "    grid2 = make_grid(denormalize(val_imgs2), nrow=samples_per_set, padding=2)\n",
        "\n",
        "    # Combine the grids vertically\n",
        "    combined_grid = torch.cat([grid1, grid2], dim=1)\n",
        "\n",
        "    # Display the combined grid\n",
        "    ax.imshow(combined_grid.permute(1, 2, 0).cpu())\n",
        "    ax.axis('off')\n",
        "    ax.set_title('Validation Pairs', fontsize=10)\n",
        "\n",
        "    # Determine dimensions for placing the labels\n",
        "    grid_width = grid1.shape[2]\n",
        "    img_width = grid_width // samples_per_set\n",
        "\n",
        "    # Add match/non-match labels for each pair\n",
        "    for i, label in enumerate(val_labels):\n",
        "        match_text = \"✓ Match\" if label == 1 else \"✗ Non-match\"\n",
        "        color = 'green' if label == 1 else 'red'\n",
        "\n",
        "        # Define a background box for the label\n",
        "        bbox_props = dict(\n",
        "            boxstyle=\"round,pad=0.3\",\n",
        "            fc=\"white\",\n",
        "            ec=color,\n",
        "            alpha=0.8\n",
        "        )\n",
        "\n",
        "        ax.text(i * img_width + img_width / 2,\n",
        "                combined_grid.shape[1] + 15,  # Position below the images\n",
        "                match_text,\n",
        "                ha='center',\n",
        "                va='top',\n",
        "                fontsize=8,\n",
        "                color=color,\n",
        "                bbox=bbox_props)\n",
        "\n",
        "    plt.suptitle(\"Verification Pairs (Top: Image 1, Bottom: Image 2)\", y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(bottom=0.05)\n",
        "    plt.show()\n",
        "\n",
        "show_ver_dataset_samples(ver_val_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3TUocDw_JU_"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FAQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4pZEjs2peC4"
      },
      "source": [
        "**What's a very low early deadline architecture (mandatory early submission)**?\n",
        "\n",
        "- The very low early deadline architecture is a 5-layer CNN. Keep in mind the parameter limit for this homework is 30M.\n",
        "- The first convolutional layer has 64 channels, kernel size 7, and stride 4. The next three have 128, 256, 512 and 1024 channels. Each have kernel size 3 and stride 2. Documentation to make convolutional layers: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "- Think about strided convolutions from the lecture, as convolutions with stride = 1 and downsampling. For strided convolution, what padding do you need for preserving the spatial resolution? (Hint => padding = kernel_size // 2) - Think why?\n",
        "- Each convolutional layer is accompanied by a Batchnorm and ReLU layer.\n",
        "- Finally, you want to average pool over the spatial dimensions to reduce them to 1 x 1. Use AdaptiveAvgPool2d. Documentation for AdaptiveAvgPool2d: https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html\n",
        "- Then, remove (Flatten?) these trivial 1x1 dimensions away.\n",
        "Look through https://pytorch.org/docs/stable/nn.html\n",
        "\n",
        "\n",
        "**Why does a very simple network have 4 convolutions**?\n",
        "\n",
        "Input images are 112x112. Note that each of these convolutions downsample. Downsampling 2x effectively doubles the receptive field, increasing the spatial region each pixel extracts features from. Downsampling 32x is standard for most image models.\n",
        "\n",
        "**Why does a very simple network have high channel sizes**?\n",
        "\n",
        "Every time you downsample 2x, you do 4x less computation (at same channel size). To maintain the same level of computation, you 2x increase # of channels, which increases computation by 4x. So, balances out to same computation. Another intuition is - as you downsample, you lose spatial information. We want to preserve some of it in the channel dimension.\n",
        "\n",
        "**What is return_feats?**\n",
        "\n",
        "It essentially returns the second-to-last-layer features of a given image. It's a \"feature encoding\" of the input image, and you can use it for the verification task. You would use the outputs of the final classification layer for the classification task. You might also find that the classification outputs are sometimes better for verification too - try both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "id": "4LLX2Rki_LzA"
      },
      "outputs": [],
      "source": [
        "# why need to add the padding\n",
        "# stride = downsampling, downsampling = 32 is standard for face recognition\n",
        "# stride / downsampling = 2, increase 2x of channels\n",
        "# You might also find that the classification outputs are sometimes better for verification too - try both.???\n",
        "class Network(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.backbone = torch.nn.Sequential(\n",
        "            # TODO\n",
        "            torch.nn.Conv2d(3, 64, kernel_size=7, stride=4, padding=3),\n",
        "            torch.nn.BatchNorm2d(64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            torch.nn.BatchNorm2d(128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "            torch.nn.BatchNorm2d(256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
        "            torch.nn.BatchNorm2d(512),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1),\n",
        "            torch.nn.BatchNorm2d(1024),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
        "        )\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        self.cls_layer = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(), \n",
        "            torch.nn.Linear(1024, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_feats=False):\n",
        "\n",
        "        feats = self.backbone(x)\n",
        "        feats = self.dropout(feats)\n",
        "        out = self.cls_layer(feats)\n",
        "\n",
        "        if return_feats:\n",
        "            return {\"feats\": feats, \"out\": out}\n",
        "        else:\n",
        "            return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deeper Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DeeperNetwork(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.backbone = torch.nn.Sequential(\n",
        "\n",
        "            torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=2, bias=False),\n",
        "            torch.nn.BatchNorm2d(64),\n",
        "            torch.nn.ReLU(),\n",
        "            \n",
        "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            torch.nn.BatchNorm2d(128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            \n",
        "            torch.nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            torch.nn.BatchNorm2d(256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            \n",
        "            torch.nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            torch.nn.BatchNorm2d(512),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            \n",
        "            torch.nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            torch.nn.BatchNorm2d(1024),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            torch.nn.Conv2d(1024, 1536, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            torch.nn.BatchNorm2d(1536),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            \n",
        "            torch.nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            )\n",
        "        \n",
        "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        self.cls_layer = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            \n",
        "            torch.nn.Linear(1536, 768),\n",
        "            torch.nn.LayerNorm(768),\n",
        "            torch.nn.ReLU(),\n",
        "            \n",
        "            torch.nn.Linear(768, num_classes)\n",
        "        )\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def forward(self, x, return_feats=False):\n",
        "        feats = self.backbone(x)\n",
        "        feats = self.dropout(feats)\n",
        "        out = self.cls_layer(feats)\n",
        "\n",
        "        if return_feats:\n",
        "            return {\"feats\": feats, \"out\": out}\n",
        "        else:\n",
        "            return out\n",
        "    \n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (torch.nn.Linear, torch.nn.Conv2d)):\n",
        "            torch.nn.init.kaiming_uniform_(m.weight, mode='fan_out', nonlinearity='relu')    \n",
        "            if hasattr(m, \"bias\") and m.bias is not None:\n",
        "                torch.nn.init.normal_(m.bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ConvNext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConvNeXtBlock(torch.nn.Module):\n",
        "    \"\"\"ConvNeXt Block with Depthwise Convolution and LayerNorm\"\"\"\n",
        "    def __init__(self, in_channels, expansion=4):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.dwconv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=7, stride=1, padding=3, groups=in_channels)  # Depthwise Conv\n",
        "        self.norm = torch.nn.GroupNorm(1, in_channels)\n",
        "        self.pwconv1 = torch.nn.Linear(in_channels, in_channels * expansion)  # Pointwise Conv (1x1)\n",
        "        self.gelu = torch.nn.GELU()\n",
        "        self.pwconv2 = torch.nn.Linear(in_channels * expansion, in_channels)  # Project back\n",
        "        self.residual = torch.nn.Identity()  # Residual Connection\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x = self.dwconv(x)  # Depthwise Convolution\n",
        "        x = self.norm(x)\n",
        "        x = x.permute(0, 2, 3, 1)  # Change (B, C, H, W) → (B, H, W, C) for LayerNorm\n",
        "        x = self.pwconv1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.pwconv2(x)\n",
        "        x = x.permute(0, 3, 1, 2)  # Convert back to (B, C, H, W)\n",
        "        return identity + x  # Residual Connection\n",
        "\n",
        "\n",
        "class ConvNeXt(torch.nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=1000, depths=[3, 3, 8, 3], dims=[80, 160, 320, 640], dropout_rate=0.2): \n",
        "        super().__init__()\n",
        "\n",
        "        self.stem = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(in_channels, dims[0], kernel_size=4, stride=4),\n",
        "            torch.nn.GroupNorm(1, dims[0])  # Changed BatchNorm to GroupNorm\n",
        "        )\n",
        "\n",
        "        self.stages = torch.nn.ModuleList()\n",
        "        for i in range(len(depths)):\n",
        "            stage = torch.nn.Sequential(\n",
        "                *[ConvNeXtBlock(dims[i]) for _ in range(depths[i])],\n",
        "                torch.nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2) if i < len(depths)-1 else torch.nn.Identity()\n",
        "            )\n",
        "            self.stages.append(stage)\n",
        "\n",
        "        self.global_avg = torch.nn.AdaptiveAvgPool2d(1)\n",
        "        \n",
        "        # Feature Extraction Layer\n",
        "        self.feature_layer = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(dims[-1], 1024),  # Output 1024-d feature embedding\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        # Classification Head\n",
        "        self.cls_layer = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(), \n",
        "            torch.nn.Linear(1024, num_classes),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x, return_feats=False):\n",
        "        x = self.stem(x)\n",
        "        for stage in self.stages:\n",
        "            x = stage(x)\n",
        "        x = self.global_avg(x)\n",
        "\n",
        "        feats = self.feature_layer(x)  # Extracted features\n",
        "        feats = self.dropout(feats)  # Apply dropout\n",
        "\n",
        "        out = self.cls_layer(feats)  # Classification Output\n",
        "\n",
        "        if return_feats:\n",
        "            return {\"feats\": feats, \"out\": out}\n",
        "        else:\n",
        "            return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BasicBlock(torch.nn.Module):\n",
        "    expansion = 1  # Used in ResNet-50+ for increasing channel depth\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.downsample = downsample  # Downsampling layer (for stride=2 blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x  # Save original input\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)  # Apply downsampling if necessary\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += identity  # Add residual connection\n",
        "        out = F.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck(torch.nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = torch.nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = torch.nn.BatchNorm2d(out_channels * self.expansion)\n",
        "        self.relu = torch.nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(torch.nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=1000, dropout_rate=0.2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            block (nn.Module): Residual block (BasicBlock or Bottleneck).\n",
        "            layers (list): Number of blocks at each layer (e.g., [2, 2, 2, 2] for ResNet-18).\n",
        "            num_classes (int): Number of output classes.\n",
        "        \"\"\"\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.in_channels = 64\n",
        "\n",
        "        # Initial convolution layer\n",
        "        self.backbone = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            torch.nn.BatchNorm2d(64),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "\n",
        "            # Residual Blocks\n",
        "            self._make_layer(block, 64, layers[0], stride=1),\n",
        "            self._make_layer(block, 128, layers[1], stride=2),\n",
        "            self._make_layer(block, 256, layers[2], stride=2),\n",
        "            self._make_layer(block, 512, layers[3], stride=2),\n",
        "\n",
        "            # Global Average Pooling & Fully Connected Layer\n",
        "            torch.nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        self.cls_layer = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(), \n",
        "            torch.nn.Dropout(p=dropout_rate),\n",
        "            torch.nn.Linear(512 * block.expansion, num_classes),\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, block, out_channels, blocks, stride):\n",
        "        \"\"\"\n",
        "        Constructs a ResNet layer with `blocks` residual blocks.\n",
        "        \"\"\"\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
        "            downsample = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                torch.nn.BatchNorm2d(out_channels * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels * block.expansion  # Update input channel size\n",
        "\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "        return torch.nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, return_feats=True):\n",
        "        feats = self.backbone(x)\n",
        "        out = self.cls_layer(feats)\n",
        "\n",
        "        if return_feats:\n",
        "            return {\"feats\": feats, \"out\": out}\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes model weights using Kaiming He initialization.\n",
        "        \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, torch.nn.Conv2d):\n",
        "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, torch.nn.BatchNorm2d):\n",
        "                torch.nn.init.constant_(m.weight, 1)\n",
        "                torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "def ResNet18(num_classes=1000, dropout_rate=0.2):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, dropout_rate)\n",
        "\n",
        "def ResNet34(num_classes=1000, dropout_rate=0.2):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes, dropout_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MobileFaceNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {},
      "outputs": [],
      "source": [
        "# channel attention, spatial attention\n",
        "\n",
        "class Conv_BN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, groups=1, activation='prelu'):\n",
        "        super(Conv_BN, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        if activation=='prelu':\n",
        "            self.activ = nn.PReLU(out_channels)\n",
        "        elif activation=='SiLU':\n",
        "            self.activ = nn.SiLU(out_channels=True)\n",
        "        else:\n",
        "            self.activ = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.bn:\n",
        "            x = self.bn(x)\n",
        "        if self.activ:\n",
        "            x = self.activ(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DepthwiseConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, activation='prelu'):\n",
        "        super(DepthwiseConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            # Depthwise convolution\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=in_channels, bias=False),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.PReLU(in_channels) if activation=='prelu' else nn.SiLU(in_channels),\n",
        "            \n",
        "            # Pointwise convolution\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16, activation='prelu'):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction, bias=False),\n",
        "            nn.PReLU(channels // reduction) if activation=='prelu' else nn.SiLU(channels // reduction),\n",
        "            nn.Linear(channels // reduction, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (N, C, H, W)\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=(kernel_size-1)//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        y = torch.cat([avg_out, max_out], dim=1)\n",
        "        y = self.conv1(y)\n",
        "        y = self.sigmoid(x)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels, channel_reduction=16, activation='prelu', spatial_kernel_size=7):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels, channel_reduction, activation)\n",
        "        self.spatial_att = SpatialAttention(spatial_kernel_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply channel attention first\n",
        "        x = self.channel_att(x)\n",
        "        # Then apply spatial attention\n",
        "        x = self.spatial_att(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride, expand_ratio, activation='prelu', use_attention=None, channel_reduction=4, spatial_kernel_size=7):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.use_res_connect = self.stride == 1 and in_channels == out_channels\n",
        "\n",
        "        hidden_dim = round(in_channels * expand_ratio)\n",
        "        \n",
        "        if expand_ratio == 1:\n",
        "            self.conv = nn.Sequential(\n",
        "                # Depthwise convolution\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.PReLU(hidden_dim) if activation=='prelu' else nn.SiLU(hidden_dim),\n",
        "                # Pointwise convolution\n",
        "                nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # Pointwise convolution to expand channels\n",
        "                nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.PReLU(hidden_dim) if activation=='prelu' else nn.SiLU(hidden_dim),\n",
        "                # Depthwise convolution\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.PReLU(hidden_dim) if activation=='prelu' else nn.SiLU(hidden_dim),\n",
        "                # Pointwise convolution to reduce channels\n",
        "                nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "        \n",
        "        if use_attention=='channel':\n",
        "            self.attention = ChannelAttention(out_channels, channel_reduction, activation)\n",
        "        elif use_attention=='spatial':\n",
        "            self.attention = SpatialAttention(spatial_kernel_size)\n",
        "        elif use_attention=='cbam':\n",
        "            self.attention = CBAM(out_channels, channel_reduction, activation, spatial_kernel_size)\n",
        "        else:\n",
        "            self.attention = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        \n",
        "        if self.attention:\n",
        "            out = self.attention(out)\n",
        "\n",
        "        if self.use_res_connect:\n",
        "            return x + out\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "class MobileFaceNet(nn.Module):\n",
        "    def __init__(self, num_classes=128, dropout_rate=0.2, activation='prelu', use_attention=False, channel_reduction=4, spatial_kernel_size=7):\n",
        "        super(MobileFaceNet, self).__init__()\n",
        "        \n",
        "        self.backbone = torch.nn.Sequential()\n",
        "\n",
        "        # Initial convolution layer, 3 x 112 -> 64 x 56\n",
        "        self.backbone.add_module(\"conv3x3\", Conv_BN(3, 64, kernel_size=3, stride=2, padding=1, activation=activation))\n",
        "        \n",
        "        # step down\n",
        "        # Depthwise convolution, 64 x 56 -> 64 x 56\n",
        "        self.backbone.add_module(\"depthwise conv3x3\", DepthwiseConv(64, 64, kernel_size=1, stride=1, padding=0, activation=activation))  # why padding=1\n",
        "        \n",
        "        # learn learn learn\n",
        "        # Bottleneck blocks, x5, 64 x 56 -> 64 x 28\n",
        "        for i in range(5):\n",
        "            if i == 0:\n",
        "                self.backbone.add_module(f\"bottleneck1_{i}\", Bottleneck(64, 64, stride=2, expand_ratio=2, activation=activation, use_attention=None))\n",
        "            else:\n",
        "                self.backbone.add_module(f\"bottleneck1_{i}\", Bottleneck(64, 64, stride=1, expand_ratio=2, activation=activation, use_attention=None))\n",
        "        \n",
        "        # step down\n",
        "        # Bottleneck blocks, x1, 64 x 28 -> 128 x 14\n",
        "        self.backbone.add_module(\"bottleneck2\", Bottleneck(64, 128, stride=2, expand_ratio=4, activation=activation, use_attention=None))\n",
        "        \n",
        "        # learn learn learn\n",
        "        # Bottleneck blocks, x6, 128 x 14 -> 128 x 14\n",
        "        for i in range(6):\n",
        "            self.backbone.add_module(f\"bottleneck3_{i}\", Bottleneck(128, 128, stride=1, expand_ratio=2, activation=activation, use_attention='channel' if use_attention else None, channel_reduction=channel_reduction))\n",
        "        \n",
        "        # step down\n",
        "        # Bottleneck blocks, x1, 128 x 14 -> 128 x 7\n",
        "        self.backbone.add_module(\"bottleneck4\", Bottleneck(128, 128, stride=2, expand_ratio=4, activation=activation, use_attention=None))\n",
        "        \n",
        "        # learn learn learn\n",
        "        # Bottleneck blocks, x2, 128 x 7 -> 128 x 7\n",
        "        for i in range(2):\n",
        "            self.backbone.add_module(f\"bottleneck5_{i}\", Bottleneck(128, 128, stride=1, expand_ratio=2, activation=activation, use_attention='cbam' if use_attention else None, channel_reduction=channel_reduction, spatial_kernel_size=spatial_kernel_size))\n",
        "       \n",
        "        # summarize from low-level to high-level\n",
        "        # Conv 1x1, 128 x 7 -> 512 x 7\n",
        "        self.backbone.add_module(\"conv1x1\", Conv_BN(128, 512, kernel_size=1, stride=1, padding=0, activation=activation))\n",
        "        \n",
        "        # avg pooling\n",
        "        # GDConv 7x7, 512 x 7 -> 512 x 1\n",
        "        self.backbone.add_module(\"linear GDConv7x7\", Conv_BN(512, 512, kernel_size=7, stride=1, padding=0, groups=512, activation=activation))\n",
        "        \n",
        "        # come up with different hypothesis\n",
        "        # Linear Conv 1x1, 512 x 1 -> 512 x 1\n",
        "        self.backbone.add_module(\"linear conv1x1\", nn.Conv2d(512, 512, kernel_size=(1,1), stride=1)),\n",
        "        \n",
        "        # Classification Layer\n",
        "        self.cls_layer = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(), \n",
        "            torch.nn.Dropout(dropout_rate),\n",
        "            torch.nn.Linear(512, num_classes),\n",
        "            torch.nn.BatchNorm1d(num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, torch.nn.Conv2d):\n",
        "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    torch.nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, torch.nn.Linear):\n",
        "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    torch.nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, (torch.nn.BatchNorm2d, torch.nn.BatchNorm1d, torch.nn.LayerNorm)):\n",
        "                torch.nn.init.constant_(m.weight, 1)\n",
        "                torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, return_feats=False):\n",
        "        # x: (N, 3, 112, 112)\n",
        "        feats = self.backbone(x)  # (N, 512, 1, 1) → flattened inside cls_layer\n",
        "        out = self.cls_layer(feats)  # (N, num_classes)\n",
        "        if return_feats:\n",
        "            return {\"feats\": feats, \"out\": out}\n",
        "        else:\n",
        "            return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize your model\n",
        "MODEL = {\n",
        "    'Network': Network,\n",
        "    'DeeperNetwork': DeeperNetwork,\n",
        "    'ConvNeXt': ConvNeXt,\n",
        "    'ResNet18': ResNet18,\n",
        "    'ResNet34': ResNet34,\n",
        "    'MobileFaceNet': MobileFaceNet, \n",
        "}\n",
        "\n",
        "model = MODEL[config['model']](num_classes=config[\"num_classes\"], **config['model_params']).to(DEVICE)\n",
        "summary(model, torch.randn(64, 3, 112, 112).to(DEVICE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CombinedMarginLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CombinedMarginLoss(torch.nn.Module):\n",
        "    def __init__(self, num_classes, embedding_size, scale=30.0, angular_margin=0.50, additive_margin=0.35, arcface_weight=0.5):\n",
        "        super(CombinedMarginLoss, self).__init__()\n",
        "        self.arcface = losses.ArcFaceLoss(config['num_classes'], embedding_size, scale=config['scale'], margin=config['angular_margin'])\n",
        "        self.cosface = losses.CosFaceLoss(config['num_classes'], embedding_size, scale=config['scale'], margin=config['additive_margin'])\n",
        "        self.arcface_weight = arcface_weight\n",
        "\n",
        "    def forward(self, embeddings, labels):\n",
        "        arcface_loss = self.arcface(embeddings, labels)\n",
        "        cosface_loss = self.cosface(embeddings, labels)\n",
        "        combined_loss = self.arcface_weight * arcface_loss + (1 - self.arcface_weight) * cosface_loss\n",
        "        return combined_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_size = model(torch.randn(64, 3, 112, 112).to(DEVICE), return_feats=True)[\"feats\"].shape[1]\n",
        "embedding_size\n",
        "\n",
        "LOSS_FUNCTIONS = {\n",
        "    \"crossentropy\": lambda config: torch.nn.CrossEntropyLoss(\n",
        "        label_smoothing=config[\"label_smoothing\"]\n",
        "    ),\n",
        "    \"triplet\": lambda config: losses.TripletMarginLoss(margin=config[\"margin\"]),\n",
        "    \"npair\": lambda config: losses.NPairsLoss(),\n",
        "    \"arcface\": lambda config: losses.ArcFaceLoss(\n",
        "        config[\"num_classes\"],\n",
        "        embedding_size,\n",
        "        scale=config[\"scale\"],\n",
        "        margin=config[\"angular_margin\"],\n",
        "    ),\n",
        "    \"cosface\": lambda config: losses.CosFaceLoss(\n",
        "        config[\"num_classes\"],\n",
        "        embedding_size,\n",
        "        scale=config[\"scale\"],\n",
        "        margin=config[\"additive_margin\"],\n",
        "    ),\n",
        "    \"combined\": lambda config: CombinedMarginLoss(\n",
        "        num_classes=config[\"num_classes\"],\n",
        "        embedding_size=embedding_size,\n",
        "        scale=config[\"scale\"],\n",
        "        angular_margin=config[\"angular_margin\"],\n",
        "        additive_margin=config[\"additive_margin\"],\n",
        "        arcface_weight=config[\"combined_arcface_weight\"],\n",
        "    ),\n",
        "}\n",
        "\n",
        "ce_criterion = LOSS_FUNCTIONS[\"crossentropy\"](config)\n",
        "if config[\"loss\"] is not None:\n",
        "    criterion = (LOSS_FUNCTIONS[config[\"loss\"]](config)).to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimizer and Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pDP--pND_3Vy"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------- #\n",
        "\n",
        "# Defining Optimizer\n",
        "# TODO: Feel free to pick a optimizer\n",
        "if config[\"loss\"] in [\"arcface\", \"cosface\", \"combined\"]:\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        [{\"params\": model.parameters()}, {\"params\": criterion.parameters()}],\n",
        "        lr=config[\"lr\"],\n",
        "        weight_decay=config[\"weight_decay\"],\n",
        "    )\n",
        "else:\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config[\"lr\"],\n",
        "        weight_decay=config[\"weight_decay\"],\n",
        "    )\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Defining Scheduler\n",
        "# TODO: Use a good scheduler such as ReduceLRonPlateau, StepLR, MultistepLR, CosineAnnealing, etc.\n",
        "SCHEDULER = {\n",
        "    \"CosineAnnealingLR\": torch.optim.lr_scheduler.CosineAnnealingLR,\n",
        "    \"CosineAnnealingWarmRestarts\": torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
        "}\n",
        "\n",
        "scheduler = SCHEDULER[config[\"scheduler\"]](optimizer, **config[\"scheduler_params\"])\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# Initialising mixed-precision training. # Good news. We've already implemented FP16 (Mixed precision training) for you\n",
        "# It is useful only in the case of compatible GPUs such as T4/V100\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d5ZDQfpw7gR"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7Ecg0J2sw9jJ"
      },
      "outputs": [],
      "source": [
        "class AverageMeter:\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TqVw0ab0xBKT"
      },
      "outputs": [],
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    maxk = min(max(topk), output.size()[1])\n",
        "    batch_size = target.size(0)\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
        "    return [correct[:min(k, maxk)].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "uNCQjz2RxD5S"
      },
      "outputs": [],
      "source": [
        "def get_ver_metrics(labels, scores, FPRs):\n",
        "    # eer and auc\n",
        "    fpr, tpr, _ = mt.roc_curve(labels, scores, pos_label=1)\n",
        "    roc_curve = interp1d(fpr, tpr)\n",
        "    EER = 100. * brentq(lambda x : 1. - x - roc_curve(x), 0., 1.) # FPR = FNR\n",
        "    AUC = 100. * mt.auc(fpr, tpr)\n",
        "\n",
        "    # get acc\n",
        "    tnr = 1. - fpr\n",
        "    pos_num = labels.count(1)\n",
        "    neg_num = labels.count(0)\n",
        "    ACC = 100. * max(tpr * pos_num + tnr * neg_num) / len(labels)\n",
        "\n",
        "    # TPR @ FPR\n",
        "    if isinstance(FPRs, list):\n",
        "        TPRs = [\n",
        "            ('TPR@FPR={}'.format(FPR), 100. * roc_curve(float(FPR)))\n",
        "            for FPR in FPRs\n",
        "        ]\n",
        "    else:\n",
        "        TPRs = []\n",
        "\n",
        "    return {\n",
        "        'ACC': ACC,\n",
        "        'EER': EER,\n",
        "        'AUC': AUC,\n",
        "        'TPRs': TPRs,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juUbZnP0AEUi"
      },
      "source": [
        "# Train and Validation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "IMnxvQT-AHsu"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "    model, dataloader, optimizer, lr_scheduler, scaler, device, epoch, loss\n",
        "):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # metric meters\n",
        "    loss_m = AverageMeter()\n",
        "    acc_m = AverageMeter()\n",
        "\n",
        "    # Progress Bar\n",
        "    batch_bar = tqdm(\n",
        "        total=len(dataloader),\n",
        "        dynamic_ncols=True,\n",
        "        leave=False,\n",
        "        position=0,\n",
        "        desc=\"Train\",\n",
        "        ncols=5,\n",
        "    )\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "\n",
        "        optimizer.zero_grad()  # Zero gradients\n",
        "\n",
        "        # send to cuda\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        if isinstance(labels, (tuple, list)):\n",
        "            targets1, targets2, lam = labels\n",
        "            labels = (targets1.to(device), targets2.to(device), lam)\n",
        "        else:\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        # forward\n",
        "        with torch.cuda.amp.autocast():  # This implements mixed precision. Thats it!\n",
        "            outputs = model(images, return_feats=True)\n",
        "\n",
        "            loss = ce_criterion(outputs[\"out\"], labels)\n",
        "            if config['loss'] is not None:\n",
        "                output_feats = F.normalize(outputs[\"feats\"].view(outputs[\"feats\"].size(0), -1))\n",
        "                other_loss = criterion(output_feats, labels)            \n",
        "                loss = loss * config['cross_entropy_weight'] + other_loss * config['other_loss_weight']\n",
        "\n",
        "            # Adversarial training part\n",
        "            if config.get('use_adversarial_training', False):\n",
        "                embeddings = output_feats.detach().clone().requires_grad_()\n",
        "                logits = model.cls_layer(embeddings, return_feats=False)\n",
        "\n",
        "                embed_adv_loss = ce_criterion(logits, labels)\n",
        "                if config['loss'] is not None:\n",
        "                    embed_adv_other_loss = criterion(embeddings, labels)\n",
        "                    embed_adv_loss = embed_adv_loss * config['cross_entropy_weight'] + embed_adv_other_loss * config['other_loss_weight']\n",
        "\n",
        "                grad = torch.autograd.grad(embed_adv_loss, embeddings)[0]\n",
        "                adv_alpha = config['adversarial_training_alpha']\n",
        "                perturbed_embeddings = embeddings + (adv_alpha * grad.sign())\n",
        "                perturbed_embeddings = F.normalize(perturbed_embeddings, p=2, dim=1)\n",
        "                perturbed_logits = model.cls_layer(perturbed_embeddings, return_feats=False)\n",
        "\n",
        "                perturbed_adv_loss = ce_criterion(perturbed_logits, labels)\n",
        "                if config['loss'] is not None:\n",
        "                    perturbed_adv_other_loss = criterion(perturbed_embeddings, labels)\n",
        "                    perturbed_adv_loss = perturbed_adv_loss * config['cross_entropy_weight'] + perturbed_adv_other_loss * config['other_loss_weight']\n",
        "\n",
        "                adv_beta = config['adversarial_training_beta']\n",
        "                loss = (1 - adv_beta) * loss + adv_beta * perturbed_adv_loss\n",
        "\n",
        "        # Backprop\n",
        "        scaler.scale(loss).backward()  # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer)  # This is a replacement for optimizer.step()\n",
        "        scaler.update()\n",
        "\n",
        "        # metrics: loss\n",
        "        loss_m.update(loss.item())\n",
        "\n",
        "        # metrics: accuracy\n",
        "        if \"feats\" in outputs:\n",
        "            acc = accuracy(outputs[\"out\"], labels)[0].item()\n",
        "        else:\n",
        "            acc = 0.0\n",
        "        acc_m.update(acc)\n",
        "\n",
        "        # tqdm lets you add some details so you can monitor training as you train.\n",
        "        batch_bar.set_postfix(\n",
        "            # acc         = \"{:.04f}%\".format(100*accuracy),\n",
        "            acc=\"{:.04f}% ({:.04f})\".format(acc, acc_m.avg),\n",
        "            loss=\"{:.04f} ({:.04f})\".format(loss.item(), loss_m.avg),\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0][\"lr\"])),\n",
        "        )\n",
        "\n",
        "        batch_bar.update()  # Update tqdm bar\n",
        "\n",
        "        # batch = epoch * len(dataloader) + (i + 1)\n",
        "        # wandb.log(\n",
        "        #     {\n",
        "        #         \"batch\": batch,\n",
        "        #         \"batch_train_loss\": loss,\n",
        "        #         \"batch_train_acc\": acc * 100,\n",
        "        #     }\n",
        "        # )\n",
        "\n",
        "    # You may want to call some schedulers inside the train function. What are these?\n",
        "    if lr_scheduler is not None:\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "    return acc_m.avg, loss_m.avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "5qkdH295wNUX"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def valid_epoch_cls(model, dataloader, device, config):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(\n",
        "        total=len(dataloader),\n",
        "        dynamic_ncols=True,\n",
        "        position=0,\n",
        "        leave=False,\n",
        "        desc=\"Val Cls.\",\n",
        "        ncols=5,\n",
        "    )\n",
        "\n",
        "    # metric meters\n",
        "    loss_m = AverageMeter()\n",
        "    acc_m = AverageMeter()\n",
        "    \n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "\n",
        "        # Move images to device\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Get model outputs\n",
        "        with torch.inference_mode():\n",
        "            outputs = model(images, return_feats=True)\n",
        "\n",
        "            loss = ce_criterion(outputs[\"out\"], labels)\n",
        "            \n",
        "            if config['loss'] is not None:\n",
        "                output_feats = F.normalize(outputs[\"feats\"].view(outputs[\"feats\"].size(0), -1))\n",
        "                other_loss = criterion(output_feats, labels)\n",
        "                loss = loss * config['cross_entropy_weight'] + other_loss * config['other_loss_weight']\n",
        "\n",
        "        # metrics\n",
        "        loss_m.update(loss.item())\n",
        "        acc = accuracy(outputs[\"out\"], labels)[0].item()\n",
        "        acc_m.update(acc)\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}% ({:.04f})\".format(acc, acc_m.avg),\n",
        "            loss=\"{:.04f} ({:.04f})\".format(loss.item(), loss_m.avg),\n",
        "        )\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "    return acc_m.avg, loss_m.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-Yan5vLDyj-3"
      },
      "outputs": [],
      "source": [
        "gc.collect() # These commands help you when you face CUDA OOM error\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q1gRMAsyknz"
      },
      "source": [
        "# Verification Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "SSGeDCi-wa1W"
      },
      "outputs": [],
      "source": [
        "def valid_epoch_ver(model, pair_data_loader, device, config):\n",
        "\n",
        "    model.eval()\n",
        "    scores = []\n",
        "    match_labels = []\n",
        "    batch_bar = tqdm(total=len(pair_data_loader), dynamic_ncols=True, position=0, leave=False, desc='Val Veri.')\n",
        "    for i, (images1, images2, labels) in enumerate(pair_data_loader):\n",
        "\n",
        "        # match_labels = match_labels.to(device)\n",
        "        images = torch.cat([images1, images2], dim=0).to(device)\n",
        "        # Get model outputs\n",
        "        with torch.inference_mode():\n",
        "            outputs = model(images, return_feats=True)\n",
        "\n",
        "        feats = F.normalize(outputs['feats'], dim=1)\n",
        "        feats1, feats2 = feats.chunk(2)\n",
        "        similarity = F.cosine_similarity(feats1, feats2)\n",
        "        scores.append(similarity.cpu().numpy().squeeze())\n",
        "        match_labels.append(labels.cpu().numpy().squeeze())\n",
        "        batch_bar.update()\n",
        "\n",
        "    scores = np.concatenate(scores)\n",
        "    match_labels = np.concatenate(match_labels)\n",
        "\n",
        "    FPRs=['1e-4', '5e-4', '1e-3', '5e-3', '5e-2']\n",
        "    metric_dict = get_ver_metrics(match_labels.tolist(), scores.tolist(), FPRs)\n",
        "    # print(metric_dict)\n",
        "\n",
        "    return metric_dict['ACC'], metric_dict['EER']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piblCbe5yotj"
      },
      "source": [
        "# WandB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTIkCXBQyoM0"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLNNqwV4ysNP"
      },
      "outputs": [],
      "source": [
        "# Create your wandb run\n",
        "if config['wandb_init']:\n",
        "    run = wandb.init(\n",
        "        name = config['wandb_name'], ## Wandb creates random run names if you skip this field\n",
        "        reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "        project = \"hw2p2\", ### Project should be created in your wandb account\n",
        "        config = config ### Wandb Config for your run\n",
        "    )\n",
        "else:\n",
        "    run = wandb.init(\n",
        "        name = config['wandb_name'], ## Wandb creates random run names if you skip this field\n",
        "        id = config['wandb_id'], ### Insert specific run id here if you want to resume a previous run\n",
        "        resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "        project = \"hw2p2\", ### Project should be created in your wandb account\n",
        "        config = config ### Wandb Config for your run\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0RrtpFKzH3k"
      },
      "source": [
        "# Checkpointing and Loading Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "O1gbAkMtlWHk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "checkpoint_dir = config['checkpoint_dir']\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "dDFmC8hpzLOq"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, metrics, epoch, best_valid_cls_acc, best_valid_ret_acc, best_valid_ret_eer, path):\n",
        "    checkpoint = {\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
        "        \"metric\": metrics,\n",
        "        \"epoch\": epoch,\n",
        "        \"best_valid_cls_acc\": best_valid_cls_acc,\n",
        "        \"best_valid_ret_acc\": best_valid_ret_acc,\n",
        "        \"best_valid_ret_eer\": best_valid_ret_eer\n",
        "    }\n",
        "\n",
        "    torch.save(checkpoint, path)\n",
        "    wandb.save(path)\n",
        "    # print(f\"Checkpoint saved locally at {path}\")\n",
        "\n",
        "\n",
        "def load_model(model, optimizer=None, scheduler=None, path=\"./checkpoint.pth\", wandb_run=True):\n",
        "    if wandb_run:\n",
        "        restored_path = wandb.restore(path).name\n",
        "        checkpoint = torch.load(restored_path, map_location=DEVICE)\n",
        "    else:\n",
        "        checkpoint = torch.load(path, map_location=DEVICE)\n",
        "\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    else:\n",
        "        optimizer = None\n",
        "    if scheduler is not None:\n",
        "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
        "    else:\n",
        "        scheduler = None\n",
        "    epoch = checkpoint[\"epoch\"]\n",
        "    metrics = checkpoint[\"metric\"]\n",
        "    best_valid_cls_acc = checkpoint[\"best_valid_cls_acc\"]\n",
        "    best_valid_ret_acc = checkpoint[\"best_valid_ret_acc\"]\n",
        "    best_valid_ret_eer = checkpoint[\"best_valid_ret_eer\"]\n",
        "\n",
        "    print(f\"Checkpoint loaded successfully (Epoch {epoch})\")\n",
        "\n",
        "    return model, optimizer, scheduler, epoch, metrics, best_valid_cls_acc, best_valid_ret_acc, best_valid_ret_eer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpFT7iriy5bi"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59FcCeJfy3Zm"
      },
      "outputs": [],
      "source": [
        "eval_cls = True\n",
        "if config['wandb_init']:\n",
        "    e = 0\n",
        "    best_valid_cls_acc = 0.0\n",
        "    best_valid_ret_acc = 0.0\n",
        "    best_valid_ret_eer = float('inf')\n",
        "else:\n",
        "    model, optimizer, scheduler, last_epoch, metrics, best_valid_cls_acc, best_valid_ret_acc, best_valid_ret_eer = load_model(model, optimizer, scheduler, \"checkpoint/last.pth\")\n",
        "    e = last_epoch + 1\n",
        "\n",
        "print(f\"loss: {config['loss']}\")\n",
        "\n",
        "for epoch in range(e, config[\"epochs\"]):\n",
        "    # epoch\n",
        "    print(f\"\\nEpoch {epoch + 1}/{config['epochs']}\")\n",
        "    metrics = {\n",
        "        \"epoch\": epoch + 1,\n",
        "    }\n",
        "\n",
        "    # train\n",
        "    train_cls_acc, train_loss = train_epoch(\n",
        "        model, cls_train_loader, optimizer, scheduler, scaler, DEVICE, epoch, config\n",
        "    )\n",
        "    curr_lr = float(optimizer.param_groups[0][\"lr\"])\n",
        "    print(\n",
        "        f\"\\nTraining Metrics - Epoch {epoch + 1}/{config['epochs']}\\n\"\n",
        "        f\"Train Cls. Acc: {train_cls_acc:.4f}%\\n\"\n",
        "        f\"Train Cls. Loss: {train_loss:.4f}\\n\"\n",
        "        f\"Learning Rate: {curr_lr:.6f}\"\n",
        "    )\n",
        "    metrics.update(\n",
        "        {\n",
        "            \"train_cls_acc\": train_cls_acc,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"lr\": curr_lr,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # classification validation\n",
        "    if eval_cls:\n",
        "        valid_cls_acc, valid_loss = valid_epoch_cls(\n",
        "            model, cls_val_loader, DEVICE, config\n",
        "        )\n",
        "        print(\n",
        "            f\"\\nClassification Validation - Epoch {epoch + 1}/{config['epochs']}\\n\"\n",
        "            f\"Val Cls. Acc: {valid_cls_acc:.4f}%\\n\"\n",
        "            f\"Val Cls. Loss: {valid_loss:.4f}\"\n",
        "        )\n",
        "        metrics.update(\n",
        "            {\n",
        "                \"valid_cls_acc\": valid_cls_acc,\n",
        "                \"valid_loss\": valid_loss,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # retrieval validation\n",
        "    valid_ret_acc, valid_ret_eer = valid_epoch_ver(\n",
        "        model, ver_val_loader, DEVICE, config\n",
        "    )\n",
        "    print(\n",
        "        f\"\\nRetrieval Validation - Epoch {epoch + 1}/{config['epochs']}\\n\"\n",
        "        f\"Val Ret. Acc: {valid_ret_acc:.4f}%\\n\"\n",
        "        f\"Val Ret. EER: {valid_ret_eer:.4f}\"\n",
        "    )\n",
        "    metrics.update(\n",
        "        {\n",
        "            \"valid_ret_acc\": valid_ret_acc,\n",
        "            \"valid_ret_eer\": valid_ret_eer,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # save best model\n",
        "    if eval_cls:\n",
        "        if valid_cls_acc >= best_valid_cls_acc:\n",
        "            best_valid_cls_acc = valid_cls_acc\n",
        "            save_model(\n",
        "                model,\n",
        "                optimizer,\n",
        "                scheduler,\n",
        "                metrics,\n",
        "                epoch,\n",
        "                best_valid_cls_acc,\n",
        "                best_valid_ret_acc,\n",
        "                best_valid_ret_eer,\n",
        "                os.path.join(config[\"checkpoint_dir\"], \"best_cls.pth\"),\n",
        "            )\n",
        "            wandb.save(os.path.join(config[\"checkpoint_dir\"], \"best_cls.pth\"))\n",
        "            print(\"Saved best classification model\")\n",
        "\n",
        "    if valid_ret_acc >= best_valid_ret_acc:\n",
        "        best_valid_ret_acc = valid_ret_acc\n",
        "        save_model(\n",
        "            model,\n",
        "            optimizer,\n",
        "            scheduler,\n",
        "            metrics,\n",
        "            epoch,\n",
        "            best_valid_cls_acc,\n",
        "            best_valid_ret_acc,\n",
        "            best_valid_ret_eer,\n",
        "            os.path.join(config[\"checkpoint_dir\"], \"best_ret_acc.pth\"),\n",
        "        )\n",
        "        wandb.save(os.path.join(config[\"checkpoint_dir\"], \"best_ret_acc.pth\"))\n",
        "        print(\"Saved best retrieval acc model\")\n",
        "\n",
        "    if valid_ret_eer <= best_valid_ret_eer:\n",
        "        best_valid_ret_eer = valid_ret_eer\n",
        "        save_model(\n",
        "            model,\n",
        "            optimizer,\n",
        "            scheduler,\n",
        "            metrics,\n",
        "            epoch,\n",
        "            best_valid_cls_acc,\n",
        "            best_valid_ret_acc,\n",
        "            best_valid_ret_eer,\n",
        "            os.path.join(config[\"checkpoint_dir\"], \"best_ret_eer.pth\"),\n",
        "        )\n",
        "        wandb.save(os.path.join(config[\"checkpoint_dir\"], \"best_ret_eer.pth\"))\n",
        "        print(\"Saved best retrieval eer model\")\n",
        "\n",
        "    # save model\n",
        "    save_model(\n",
        "        model,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        metrics,\n",
        "        epoch,\n",
        "        best_valid_cls_acc,\n",
        "        best_valid_ret_acc,\n",
        "        best_valid_ret_eer,\n",
        "        os.path.join(config[\"checkpoint_dir\"], \"last.pth\"),\n",
        "    )\n",
        "    wandb.save(os.path.join(config[\"checkpoint_dir\"], \"last.pth\"))\n",
        "\n",
        "    # log to tracker\n",
        "    if run is not None:\n",
        "        run.log(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXLTfQjv0cCb"
      },
      "source": [
        "# Testing and Kaggle Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "XUAa3m2h0eCD"
      },
      "outputs": [],
      "source": [
        "def test_epoch_ver(model, pair_data_loader, config):\n",
        "\n",
        "    model.eval()\n",
        "    scores = []\n",
        "    batch_bar = tqdm(total=len(pair_data_loader), dynamic_ncols=True, position=0, leave=False, desc='Val Veri.')\n",
        "    for i, (images1, images2) in enumerate(pair_data_loader):\n",
        "\n",
        "        images = torch.cat([images1, images2], dim=0).to(DEVICE)\n",
        "        # Get model outputs\n",
        "        with torch.inference_mode():\n",
        "            outputs = model(images, return_feats=True)\n",
        "\n",
        "        feats = F.normalize(outputs['feats'], dim=1)\n",
        "        feats1, feats2 = feats.chunk(2)\n",
        "        similarity = F.cosine_similarity(feats1, feats2)\n",
        "        scores.extend(similarity.cpu().numpy().tolist())\n",
        "        batch_bar.update()\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission_dir = \"submissions\"\n",
        "os.makedirs(submission_dir, exist_ok=True)\n",
        "\n",
        "model_paths = [\"best_ret_eer.pth\", \"best_ret_acc.pth\", \"last.pth\"]\n",
        "\n",
        "for model_path in model_paths:\n",
        "    print(f\"Loading {model_path}\")\n",
        "\n",
        "    model, optimizer, scheduler, last_epoch, metrics, best_valid_cls_acc, best_valid_ret_acc, best_valid_ret_eer = load_model(model, optimizer, scheduler, os.path.join(config['checkpoint_dir'],model_path))\n",
        "    print(f\"best cls acc: {best_valid_cls_acc}, best ret acc: {best_valid_ret_acc}, best ret eer: {best_valid_ret_eer}\")\n",
        "\n",
        "    scores = test_epoch_ver(model, ver_test_loader, config)\n",
        "    scores = torch.flatten(torch.tensor(scores))\n",
        "\n",
        "    file_name = os.path.join(submission_dir, model_path.split(\".\")[0] + '_submission.csv')\n",
        "\n",
        "    with open(file_name, \"w+\") as f:\n",
        "        f.write(\"ID,Label\\n\")\n",
        "        for i in range(len(scores)):\n",
        "            f.write(\"{},{}\\n\".format(i, scores[i]))\n",
        "\n",
        "    artifact = wandb.Artifact(\"submission\", type=\"dataset\")\n",
        "    artifact.add_file(file_name)\n",
        "    wandb.log_artifact(artifact)\n",
        "    print(model_path.split(\".\")[0] + \" submitted to WandB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "v3_8VUTQ52zT"
      },
      "outputs": [],
      "source": [
        "# ### Submit to kaggle competition using kaggle API (Uncomment below to use)\n",
        "# !kaggle competitions submit -c 11785-hw-2-p-2-face-verification-spring-2025 -f /content/verification_early_submission.csv -m \"Test Submission\"\n",
        "\n",
        "# ### However, its always safer to download the csv file and then upload to kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
